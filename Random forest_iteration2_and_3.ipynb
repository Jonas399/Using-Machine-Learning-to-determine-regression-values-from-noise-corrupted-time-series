{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"Random forest.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ivpL27iBlbHT","executionInfo":{"status":"ok","timestamp":1630759224546,"user_tz":-120,"elapsed":18389,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}},"outputId":"a395f74d-2e8d-4cd6-855e-249c8b00d54f"},"source":["#@title\n","from google.colab import drive\n","drive.mount('/content/drive')"],"id":"ivpL27iBlbHT","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"vUYusVlEkJnB","executionInfo":{"status":"ok","timestamp":1630759231029,"user_tz":-120,"elapsed":3075,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}}},"source":["#@title\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import sklearn as skl\n","from sklearn import preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from scipy import ndimage\n","\n","# changes data into Pytorch tensors, so they can be used for models\n","def createTrainTensors(train):\n","    # Train data\n","    np_train_data = train.to_numpy()\n","    x = torch.from_numpy(np_train_data.astype(np.float32))\n","    return x\n","\n","\n","def createParamsTensors(train, params):\n","    # Params data\n","    np_params_data = params.to_numpy()\n","    np_params_data_reshape = np_params_data.reshape(len(train.T), 1)\n","    # np_params_data_reshape = np_params_data.reshape(500, 55,1)\n","\n","    z = torch.from_numpy(np_params_data_reshape)  # .astype)#(np.float32))\n","\n","    return z\n","\n","\n","def createParamsNumpy(train, params):\n","    np_params_data = params.to_numpy()\n","    y = np_params_data.reshape(len(train.T), 1)\n","    # y = np_params_data.reshape(500, 55 ,1)\n","\n","    return y\n","\n","\n","def createTestTensors(test):\n","    # Test data\n","    if len(test) > 0:\n","        np_test_data = test.to_numpy()\n","        x_test = torch.from_numpy(np_test_data.astype(np.float32))\n","\n","\n","#Calculates median for dataframe\n","def calculateMedianValue(data):\n","\n","    #medianData = data.T\n","    #medianData = medianData.rolling(3).median()\n","    #medianData = medianData.T\n","    #medianData = medianData.fillna(1)\n","\n","    medianData = ndimage.median_filter(data, 3)\n","    return medianData\n","\n","#first 30 values are replaced by the value of 1, based on the baseline solution\n","def replaceFirstXValues(data):\n","    for i in range(10):\n","        data[i] = 1\n","    return data\n","\n","#removes first x values from dataframe\n","def removeFirstValues(data, to_remove):\n","    if type(data) != np.ndarray:\n","        for i in range(to_remove):\n","            data = data.drop(labels=i, axis=1)\n","\n","    else:\n","        data = data[:, to_remove:]\n","\n","    return data\n","#normalizes values for each measurement(1-300)\n","def normalizeDataFrame(data_frame):\n","\n","    values = data_frame  # .values\n","    values = values.T\n","    min_max_scaler = preprocessing.MinMaxScaler()\n","    scaled_values = min_max_scaler.fit_transform(values)\n","    scaled_values = scaled_values.T\n","\n","    norm_data = pd.DataFrame(scaled_values)\n","    return norm_data\n","\n","def replaceValuesOverOne(data_frame):\n","    cleaned_list = []\n","    for element in data_frame:\n","        for value in element:\n","            if value > 1:\n","                cleaned_list.append(1)\n","            else:\n","                cleaned_list.append(value)\n","    np_cleaned = np.array(cleaned_list)\n","    np_cleaned = np_cleaned.reshape(len(np_cleaned),1)\n","    return np_cleaned\n","\n","def zScoreDataFrame(data_frame, mean_value, standard_deviation):\n","    if type(data_frame) != np.ndarray:\n","       data_frame = data_frame.to_numpy()\n","\n","    #mean_value = np.mean(data_frame)\n","    #standard_deviation = np.std(data_frame)\n","\n","    scaled_data_frame = (data_frame - mean_value) / standard_deviation\n","    #print(f\"Rescaling data with mean = {mean_value}, std = {standard_deviation}\")\n","    return scaled_data_frame\n","\n","def rescaleData(data_frame, mean_value, standard_deviation):\n","    data_unscaled = data_frame * standard_deviation + mean_value\n","    return data_unscaled\n","\n","def preprocessData(data_frame, median, replaceValuesBiggerOne, replaceFirstXValues, removeFirstXValues,\n","                   normalizeData, zScore, zScore_mean, zScore_sd, number_to_remove):\n","\n","    if median == True:\n","        data_frame = calculateMedianValue(data_frame)\n","    if replaceValuesBiggerOne == True:\n","        data_frame = data_frame.apply(replaceValuesOverOne)\n","    if replaceFirstXValues == True:\n","        data_frame = replaceFirstXValues(data_frame)\n","    if removeFirstXValues == True:\n","        data_frame = removeFirstValues(data_frame, number_to_remove)\n","    if normalizeData == True:\n","        data_frame = normalizeDataFrame(data_frame)\n","    if zScore == True:\n","        data_frame = zScoreDataFrame(data_frame, zScore_mean, zScore_sd)\n","\n","    return data_frame\n","\n","def preprocessParams(data_frame):\n","    data_frame = normalizeDataFrame(data_frame)\n","\n","    return data_frame\n","\n","def preprocessParameters(data):\n","    zscore_scaler = StandardScaler()\n","    data = zscore_scaler.fit_transform(data)\n","\n","    return data\n"],"id":"vUYusVlEkJnB","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"hElnpzd4kJgp","executionInfo":{"status":"ok","timestamp":1630759236506,"user_tz":-120,"elapsed":2373,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}}},"source":["#@title\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow import keras\n","from tensorflow.keras import layers, activations\n","from tensorflow.keras.layers import Dropout\n","\n","\n","# Implements scoring system for evaluation\n","def scoringSystem(y_true, y_pred):\n","    #print(type(y_true))\n","\n","    weight = tf.ones_like(y_true)\n","    absolute = tf.math.abs(y_pred - y_true)\n","    # neuer stand\n","    #print(\"absolut: \", absolute)\n","    counter = tf.math.reduce_sum((weight * 2 * y_true * absolute))\n","    denominator = tf.math.reduce_sum(weight)\n","    print(denominator)\n","\n","    formula = 1e4 - (counter / denominator) * 1e6\n","\n","    return formula\n","\n","class simpleRegression(torch.nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(simpleRegression, self).__init__()\n","        self.l1 = nn.Linear(input_size, output_size)\n","\n","    def forward(self, x):\n","        out = self.l1(x)\n","        return out\n","\n","\n","class feedForward(torch.nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(feedForward, self).__init__()\n","        #self.flatten = nn.Flatten()\n","        self.l1 = nn.Linear(input_size, 150)\n","        self.relu = nn.ReLU()\n","        self.l2 = nn.Linear(150, output_size)\n","\n","    def forward(self, x):\n","        #out = self.flatten(x)\n","        out = self.l1(x)\n","        out = self.relu(out)\n","        out = self.l2(out)\n","        return out\n","\n","\n","def sequentialModel(inputs, lr_rate):\n","    model = keras.Sequential([\n","        keras.layers.InputLayer((55, 300)),\n","        keras.layers.Flatten(),\n","       # layers.Activation(activations.selu),\n","       # layers.Dense(2048),\n","       # layers.Activation(activations.selu),\n","       # layers.Dense(1024),\n","        #layers.Activation(activations.selu),\n","        #layers.Dense(512),\n","        layers.Activation(activations.selu),\n","       # layers.Dense(256),\n","       # layers.Activation(activations.selu),\n","        layers.Dense(128),\n","        layers.Activation(activations.selu),\n","        layers.Dense(64),\n","        layers.Activation(activations.selu),\n","        layers.Dense(55)\n","    ])\n","\n","    optimizer = keras.optimizers.Adamax()\n","\n","    model.compile(loss=\"mse\",\n","                  optimizer=optimizer,\n","                  metrics=[scoringSystem])\n","\n","    return model\n","\n","\n","def sequentialDropout(inputs, lr_rate):\n","    dropout_rate = 0.3\n","    model = keras.Sequential([\n","        layers.Dropout(dropout_rate),\n","        keras.layers.InputLayer((55, 300)),\n","        keras.layers.Flatten(),\n","        #layers.Dropout(dropout_rate),\n","        layers.Activation(activations.selu),\n","        layers.Dense(256),\n","        layers.Activation(activations.selu),\n","        #layers.Dropout(dropout_rate),\n","        layers.Dense(128),\n","        layers.Activation(activations.selu),\n","        #layers.Dropout(dropout_rate),\n","        layers.Dense(64),\n","        layers.Activation(activations.selu),\n","        #layers.Dropout(dropout_rate),\n","        #layers.Dense(32),\n","        #layers.Dropout(dropout_rate),\n","        layers.Dense(55)\n","    ])\n","\n","    optimizer = keras.optimizers.Adam(learning_rate=lr_rate)\n","\n","    model.compile(loss=\"mse\",\n","                  optimizer=optimizer,\n","                  metrics=[scoringSystem])\n","    return model\n","\n","\n","def lstmModel(inputs):\n","    model = keras.Sequential([\n","        layers.LSTM(inputs),\n","        layers.LSTM(1)\n","    ])\n","    optimizer = keras.optimizers.Adam()\n","\n","    model.compile(loss=\"mse\",\n","                  optimizer=optimizer,\n","                  metrics=[scoringSystem])\n","\n","    return model\n","\n","\n","def cnnModel(number_of_measurements):\n","    model = keras.Sequential([\n","        keras.layers.InputLayer(input_shape=(55, number_of_measurements)),\n","        layers.Conv1D(filters=32, kernel_size=5),\n","        layers.MaxPooling1D(pool_size=2),\n","        layers.Activation(activations.selu),\n","        layers.Conv1D(filters=32, kernel_size=5),\n","        layers.MaxPooling1D(pool_size=3),\n","        layers.Activation(activations.selu),\n","        layers.Conv1D(filters=32, kernel_size=5),\n","        layers.MaxPooling1D(pool_size=3),\n","        layers.Flatten(),\n","        layers.Dense(1024),\n","        layers.Activation(activations.selu),\n","        layers.Dense(1024),\n","        layers.Activation(activations.selu),\n","        layers.Dense(1024),\n","        layers.Activation(activations.selu),\n","        layers.Dense(1024),\n","        layers.Activation(activations.selu),\n","        layers.Dense(55),\n","    ])\n","    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n","\n","    model.compile(loss=\"mse\",\n","                  optimizer=optimizer,\n","                  metrics=[scoringSystem])\n","\n","    return model\n","\n","\n","def cnnModelFilters(number_of_measurements):\n","    model = keras.Sequential([\n","        keras.layers.InputLayer(input_shape=(55, number_of_measurements)),\n","        layers.Conv1D(filters=32, kernel_size=5),\n","        layers.MaxPooling1D(pool_size=2),\n","        layers.Activation(activations.selu),\n","        layers.Conv1D(filters=32, kernel_size=5),\n","        layers.MaxPooling1D(pool_size=2),\n","        layers.Activation(activations.selu),\n","        layers.Conv1D(filters=32, kernel_size=5),\n","        layers.MaxPooling1D(pool_size=2),\n","        layers.Flatten(),\n","        layers.Dense(1024),\n","        layers.Activation(activations.selu),\n","        layers.Dense(512),\n","        layers.Activation(activations.selu),\n","        layers.Dense(256),\n","        layers.Activation(activations.selu),\n","        layers.Dense(128),\n","        layers.Activation(activations.selu),\n","        layers.Dense(55),\n","    ])\n","    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n","\n","    model.compile(loss=\"mae\",\n","                  optimizer=optimizer,\n","                  metrics=[scoringSystem])\n","\n","    return model"],"id":"hElnpzd4kJgp","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"1GrBz8DDkJZM","executionInfo":{"status":"ok","timestamp":1630759237216,"user_tz":-120,"elapsed":2,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}}},"source":["#@title\n","import pandas as pd\n","import numpy as np\n","\n","\n","def importTrainingData(files, stop_read, count):\n","    # Import Training files\n","    if count%2 == 0:\n","        print(\"Importing Training Data...\", \"Iteration \", count)\n","    else:\n","        print(\"Importing Validation Data...\", \"Iteration \", count-1)\n","    training_list = []\n","\n","    # Add first n files to list\n","    for filename in files:\n","        df = pd.read_csv(filename, header=None, sep=\"\\t\", skiprows=6)\n","        training_list.append(df)\n","        count = count + 1\n","        if count == stop_read:\n","            break\n","\n","    # Convert List content to Dataframe\n","    train_data = pd.concat(training_list, axis=0, ignore_index=True)\n","\n","    return train_data\n","\n","\n","def importParamsData(files, stop_read, count):\n","    # Import Params\n","    if count % 2 == 0:\n","        print(\"Importing Training Params...\", \"Iteration \", count)\n","    else:\n","        print(\"Importing Validation Params...\", \"Iteration \", count - 1)\n","    params_list = []\n","\n","    # Add first n files to list\n","    for filename in files:\n","        df = pd.read_csv(filename, header=None, sep=\"\\t\", skiprows=2)\n","        params_list.append(df)\n","        count = count + 1\n","        if count == stop_read:\n","            break\n","\n","    # Convert List content to Dataframe\n","    params_data = pd.concat(params_list, axis=0, ignore_index=True)\n","\n","    return params_data\n","\n","def importTestData(test_files):\n","    li = []\n","\n","    for filename in test_files:\n","        df = pd.read_csv(filename, header=None, sep=\"\\t\", skiprows=6)\n","        li.append(df)\n","\n","    test_frame = pd.concat(li, axis=0, ignore_index=True)\n","\n","    return test_frame\n","\n","def importTargetParameter(files, stop_read, count):\n","    train_parameters = []\n","    # Add first n files to list\n","    for filename in files:\n","        df = pd.read_csv(filename, header=None, sep=\" \", nrows=2)\n","        train_parameters.append(df)\n","        count = count + 1\n","        if count == stop_read:\n","            break\n","\n","    # Convert List content to Dataframe\n","    parameter_data = pd.concat(train_parameters, axis=0, ignore_index=True)\n","\n","    return parameter_data\n","\n","\n","def importTrainParameter(files, stop_read, count):\n","    train_parameters = []\n","\n","    # Add first n files to list\n","    for filename in files:\n","        df = pd.read_csv(filename, header=None, sep=\" \", nrows=6)\n","        for x in range(55):\n","            train_parameters.append(df)\n","        count = count + 1\n","        if count == stop_read:\n","            break\n","\n","    # Convert List content to Dataframe\n","    parameter_data = pd.concat(train_parameters, axis=0, ignore_index=True)\n","\n","    return parameter_data\n","\n","def importTrainParameterSingle(files, stop_read, count):\n","    train_parameters = []\n","\n","    # Add first n files to list\n","    for filename in files:\n","        df = pd.read_csv(filename, header=None, sep=\" \", nrows=6)\n","        #for x in range(55):\n","        train_parameters.append(df)\n","        count = count + 1\n","        if count == stop_read:\n","            break\n","\n","    # Convert List content to Dataframe\n","    parameter_data = pd.concat(train_parameters, axis=0, ignore_index=True)\n","\n","    return parameter_data\n","\n","def importEvalParameter(files):\n","    eval_parameters = []\n","\n","    # Add first n files to list\n","    for filename in files:\n","        df = pd.read_csv(filename, header=None, sep=\" \", nrows=6)\n","        for x in range(55):\n","            eval_parameters.append(df)\n","\n","    # Convert List content to Dataframe\n","    parameter_data = pd.concat(eval_parameters, axis=0, ignore_index=True)\n","\n","    return parameter_data"],"id":"1GrBz8DDkJZM","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a27d79f3","executionInfo":{"status":"ok","timestamp":1630759307561,"user_tz":-120,"elapsed":68127,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}},"outputId":"53ea8488-5f65-4351-d620-99ba9b06bd6d"},"source":["#@title\n","#Tensorflow\n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras import layers, activations\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.optimizers import *\n","from sklearn.ensemble import RandomForestRegressor\n","\n","#Pandas and numpy for data formats\n","import pandas as pd\n","import numpy as np\n","\n","#glob for data import\n","import glob\n","import random\n","#PyTorch\n","import torch\n","import torch.nn as nn\n","#SK Learn\n","import sklearn as skl\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from sklearn.preprocessing import StandardScaler\n","#from sklearn.utils import shuffle\n","import math\n","import sys\n","#MatPlotLib\n","import matplotlib.pyplot as plt\n","\n","#importTrainData()\n","train_data = np.load(\"/content/drive/MyDrive/BA_data/Data 40000/train_40000.npy\")\n","params_data = np.load(\"/content/drive/MyDrive/BA_data/Data 40000/train_target_40000.npy\")\n","#train_parameter = np.load(\"../train_parameter_40000.npy\")\n","#\n","validation_data = np.load(\"/content/drive/MyDrive/BA_data/Data 40000/val_train_40000.npy\")\n","validation_params = np.load(\"/content/drive/MyDrive/BA_data/Data 40000/val_target_40000.npy\")\n","#validation_parameter = np.load(\"../val_parameter_40000.npy\")\n","\n","#Create stats for zScore\n","mean_value_target = np.mean(params_data)\n","standard_deviation_target = np.std(params_data)\n","\n","mean_value = np.mean(train_data)\n","standard_deviation = np.std(train_data)\n","measurements_to_remove = 30\n","\n","\n","median_train = False\n","replaceValuesBiggerOne_train=False\n","replaceFirstXValues_train=False\n","removeFirstXValues_train=True\n","normalizeData_train=False\n","zScore_train=True\n","\n","train_data = preprocessData(train_data,\n","                          median=median_train,\n","                          replaceValuesBiggerOne = replaceValuesBiggerOne_train,\n","                          replaceFirstXValues = replaceFirstXValues_train,\n","                          removeFirstXValues = removeFirstXValues_train,\n","                          normalizeData = normalizeData_train,\n","                          zScore = zScore_train,\n","                          zScore_mean = mean_value,\n","                          zScore_sd = standard_deviation,\n","                          number_to_remove = measurements_to_remove)\n","\n","train_target = preprocessData(params_data,\n","                          median=False,\n","                          replaceValuesBiggerOne=False,\n","                          replaceFirstXValues=False,\n","                          removeFirstXValues=False,\n","                          normalizeData=False,\n","                          zScore=False,\n","                          zScore_mean=mean_value_target,\n","                          zScore_sd=standard_deviation_target,\n","                          number_to_remove=measurements_to_remove)\n","\n","validation_data = preprocessData(validation_data,\n","                          median = median_train,\n","                          replaceValuesBiggerOne = replaceValuesBiggerOne_train,\n","                          replaceFirstXValues = replaceFirstXValues_train,\n","                          removeFirstXValues = removeFirstXValues_train,\n","                          normalizeData = normalizeData_train,\n","                          zScore = zScore_train,\n","                          zScore_mean = mean_value,\n","                          zScore_sd = standard_deviation,\n","                          number_to_remove = measurements_to_remove)\n","\n","validation_target = preprocessData(validation_params,\n","                          median=False,\n","                          replaceValuesBiggerOne=False,\n","                          replaceFirstXValues=False,\n","                          removeFirstXValues=False,\n","                          normalizeData=False,\n","                          zScore=False,\n","                          zScore_mean=mean_value_target,\n","                          zScore_sd=standard_deviation_target,\n","                          number_to_remove=measurements_to_remove)\n","\n","\n","number_of_measurements = 300\n","if removeFirstXValues_train == True:\n","    number_of_measurements = 300 - measurements_to_remove    \n","    \n","#reshaping train data\n","if type(train_data) != np.ndarray:\n","    train_data = train_data.to_numpy().reshape(-1, 55, number_of_measurements)\n","else:\n","    train_data = train_data.reshape(-1, 55, number_of_measurements)\n","\n","#reshaping target data\n","if type(params_data) != np.ndarray:\n","    params_data = params_data.to_numpy()\n","\n","#reshaping val data\n","if type(validation_data) != np.ndarray:\n","    validation_data = validation_data.to_numpy().reshape(-1, 55, number_of_measurements)\n","else:    \n","    validation_data = validation_data.reshape(-1, 55, number_of_measurements)\n","\n","#reshaping val target\n","if type(validation_params) != np.ndarray:\n","    validation_params = validation_params.to_numpy()\n","\n","    \n","def scaleToMaxToOne(dat, min, max):\n","    scaled_dat = dat / max\n","    return scaled_dat\n","\n","def unscaleMaxToOne(scaled_dat, min, max):\n","    unscaled_dat = scaled_dat * max\n","    return unscaled_dat\n","\n","min_target = np.min(train_target) \n","max_target = np.max(train_target)\n","print(f\"Rescaling targets with max = {max_target}\")\n","train_target = scaleToMaxToOne(train_target, min_target, max_target)\n","val_target = scaleToMaxToOne(validation_target, min_target, max_target)    \n","\n","prevent_overfitting = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience = 20, restore_best_weights=True)\n","reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, min_delta=0.0001)"],"id":"a27d79f3","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Rescaling targets with max = 0.6713607926990712\n"]}]},{"cell_type":"code","metadata":{"id":"15edc9d7","executionInfo":{"status":"ok","timestamp":1630759307562,"user_tz":-120,"elapsed":6,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}}},"source":["train_data_reshaped = train_data.reshape(2200000, 270)"],"id":"15edc9d7","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"9cf913da","executionInfo":{"status":"ok","timestamp":1630759307563,"user_tz":-120,"elapsed":6,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}}},"source":["train_target_reshaped = train_target.reshape(2200000,)"],"id":"9cf913da","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfbb02be","executionInfo":{"status":"ok","timestamp":1630759307564,"user_tz":-120,"elapsed":5,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}}},"source":["validation_data_reshaped = validation_data.reshape(550000, 270)"],"id":"cfbb02be","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DwYrCS3J1N7P"},"source":["##Model 1"],"id":"DwYrCS3J1N7P"},{"cell_type":"code","metadata":{"id":"243b5f5f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630770567562,"user_tz":-120,"elapsed":11260003,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}},"outputId":"4390a309-e3b9-4683-c546-e4e9d8a50e1e"},"source":["rfr = RandomForestRegressor(n_estimators = 100, verbose=1, n_jobs = -1)\n","\n","rfrModel = rfr.fit(train_data_reshaped, train_target_reshaped)\n"],"id":"243b5f5f","execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 187.6min finished\n"]}]},{"cell_type":"code","metadata":{"id":"c6UYQC79Etje","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630770575906,"user_tz":-120,"elapsed":8349,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}},"outputId":"0db40b13-c479-4ee8-d391-559cecf6da1d"},"source":["prediction = rfrModel.predict(validation_data_reshaped)"],"id":"c6UYQC79Etje","execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n","[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    5.8s finished\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V50X1m8b0Ssv","executionInfo":{"status":"ok","timestamp":1630770576289,"user_tz":-120,"elapsed":384,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}},"outputId":"5eea9009-1fe5-4959-f5de-6958a2ccf423"},"source":["pred_final = unscaleMaxToOne(prediction, min_target, max_target)\n","\n","pred_final_reshape = pred_final.reshape(10000,55)\n","\n","score = scoringSystem(validation_target, pred_final_reshape)\n","print(score)"],"id":"V50X1m8b0Ssv","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(550000.0, shape=(), dtype=float64)\n","tf.Tensor(9127.16029835004, shape=(), dtype=float64)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SYO_ogKc6A3R","executionInfo":{"status":"ok","timestamp":1630770578383,"user_tz":-120,"elapsed":344,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}},"outputId":"c0ed0dc0-c871-410b-f714-4c070d9ec282"},"source":["import gc\n","del rfr\n","gc.collect()"],"id":"SYO_ogKc6A3R","execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["185"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"HCxinlmx5mwy"},"source":["##Model 2"],"id":"HCxinlmx5mwy"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9BuLf0Qi5mVd","executionInfo":{"status":"ok","timestamp":1630782147419,"user_tz":-120,"elapsed":11567556,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}},"outputId":"ac15e9e9-1b06-482d-a554-aae00e6ff7f4"},"source":["rfr2 = RandomForestRegressor(n_estimators = 100, verbose=1, n_jobs = -1)\n","\n","rfr2Model = rfr2.fit(train_data_reshaped, train_target_reshaped)\n","prediction2 = rfr2Model.predict(validation_data_reshaped)"],"id":"9BuLf0Qi5mVd","execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 192.6min finished\n","[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n","[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    4.1s finished\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXIwu2Eu5_wi","executionInfo":{"status":"ok","timestamp":1630782147750,"user_tz":-120,"elapsed":338,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}},"outputId":"5344d52d-cf36-4ec7-e700-8d15e11c3f83"},"source":["pred_final2 = unscaleMaxToOne(prediction2, min_target, max_target)\n","\n","pred_final2_reshape = pred_final2.reshape(10000,55)\n","\n","score2 = scoringSystem(validation_target, pred_final2_reshape)\n","print(score2)"],"id":"hXIwu2Eu5_wi","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(550000.0, shape=(), dtype=float64)\n","tf.Tensor(9132.09957218906, shape=(), dtype=float64)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjK1Jfa16XoC","executionInfo":{"status":"ok","timestamp":1630782148229,"user_tz":-120,"elapsed":188,"user":{"displayName":"Sven Ebert","photoUrl":"","userId":"03010888979282115588"}},"outputId":"be19b863-50a5-45c7-96f5-00a9225142d4"},"source":["del rfr2\n","gc.collect()"],"id":"SjK1Jfa16XoC","execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["118"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"ZJeI75-U6ZBy"},"source":["##Model 3"],"id":"ZJeI75-U6ZBy"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v4ZWWV_z6cSR","outputId":"8982d722-7adb-4e5b-92cb-785cffe98fc1"},"source":["rfr3 = RandomForestRegressor(n_estimators = 100, verbose=1, n_jobs = -1)\n","\n","rfr3Model = rfr3.fit(train_data_reshaped, train_target_reshaped)\n","prediction3 = rfr3Model.predict(validation_data_reshaped)"],"id":"v4ZWWV_z6cSR","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n"]}]},{"cell_type":"code","metadata":{"id":"dt5dkFN_7KUL"},"source":["pred_final3 = unscaleMaxToOne(prediction3, min_target, max_target)\n","\n","pred_final3_reshape = pred_final3.reshape(10000,55)\n","\n","score3 = scoringSystem(validation_target, pred_final3_reshape)\n","print(score3)"],"id":"dt5dkFN_7KUL","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wbnQ1O1E7cbh"},"source":["del rfr3\n","gc.collect()"],"id":"wbnQ1O1E7cbh","execution_count":null,"outputs":[]}]}